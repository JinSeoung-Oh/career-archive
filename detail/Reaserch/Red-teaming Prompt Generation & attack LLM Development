프로젝트명: Red-teaming Prompt Generation & attack LLM Development
기간: 2024년 3월 ~ 5월

1) 목적
   - LLM이 안전하지 않은 응답을 생성하도록 유도하는 레드티밍 프롬프트 기법을 체계화.
   - 이를 자동화할 수 있는 레드티밍 전용 LLM을 개발하여 보안 검증 및 방어 모델(Guard) 평가에 활용.

2) 방법
   (1) Red-teaming Prompt Guide 작성 (3월)
       -1. 공격 기법 분류:
           - 해로운 질문 직접 생성
           - 모호성 증가 (“아니 그게 아니고…” 반복, 다중 해석 유도)
           - 잡음 삽입
           - 잘못된 정보 제공을 통한 유사도 조작
       -2. 대화 기반 공격 단계 정의:
           - Topic 선정 (소주제 단위, 최소 5턴 이상 대화 가능성 고려)
           - Red-Questions 생성 (민감/비윤리적 질문 리스트업)
           - Harmful answer 유도를 위한 대화 진행 (가정적 시나리오, 페르소나 기법, 사회공학적 설정)
           - QA 형태로 재정리하여 평가/활용 가능하도록 변환
      핵심 원리: LLM의 확률적 생성 특성을 이용해, 정상 대화 중 비윤리적 질문을 교묘하게 삽입하여 guardrail을 우회하도록 설계.

(2) Red-teaming LLM 구현 (5월)
    -1. GitHub 오픈소스(JailbreakingLLMs) 기반으로 레드티밍 자동화 실험.
    -2. 레드티밍 전용 LLM 구성:
        - 입력된 주제/시나리오에 따라 adversarial prompt와 대화 시퀀스를 자동 생성.
        - 최소 5턴 이상의 대화 구조를 유지하며 jailbreak 성공 확률을 높임.
        - 레포 호환성 문제 수정, 데이터 저장 및 재사용 가능한 코드 추가.
    결과: 레드티밍 LLM을 이용해 다양한 공격 시나리오를 자동 생성 가능함을 확인.

3) 성과
   - 체계적 가이드라인 + 실제 동작하는 레드티밍 LLM을 동시에 확보.
   - 보안 검증 과정에서 수작업에 의존하던 red-teaming을 자동화할 수 있는 기반 마련.
   - 추후 Guard 모델 학습/평가, 멀티모달 모델 보안 검증 등으로 확장 가능성 확인.

4) 인사이트
   - Red-teaming은 단순 프롬프트 공격이 아니라 대화 맥락 기반, 사회공학적 유도 기법이 가장 효과적임.
   - LLM 방어(Guardrail) 강화에는 Red-teaming LLM이 지속적으로 공격 데이터를 만들어내는 구조가 필요.
   - 멀티모달 모델(Vision-Language Model 등)로 확장 시 더 강력한 취약점 검증 도구로 발전 가능.
