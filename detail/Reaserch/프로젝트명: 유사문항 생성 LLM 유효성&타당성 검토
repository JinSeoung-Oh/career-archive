프로젝트명: 유사문항 생성 LLM 유효성/타당성 검토
기간: 2024년 4월

1) 목적
   - 교육 분야에서 LLM을 활용해 기출 문제 기반의 유사 문항을 자동 생성할 수 있는지 검증. 
   - 모델의 유효성(Validity)과 타당성(Reliability)을 교과별로 점검하여 실제 적용 가능성을 판단.

2) 방법
   - 모델: GPT-4 (gpt-4-0613), GPT-3.5, Claude-3.5
   - 데이터: 2024학년도 수능 기출문제(국어·수학·국사·과학·영어)
   - 실험 조건:
     사람이 직접 입력 혹은 OCR 파싱된 문제를 입력했다고 가정
     “유사 문제 10개 생성” 등 요구사항을 동일하게 제시
     객관식 ↔ 주관식 변형 요청 포함

3) 결과
   - 수학: GPT-4/3.5 모두 숫자 치환 문제 위주 생성. “유사 문제”를 개념 동일/유사 여부에 따라 다르게 해석 → 결과의 타당성은 정의에 따라 달라짐.
   - 국어: GPT-4는 일정 수준 문항 변형 가능, GPT-3.5는 중복·잘못된 텍스트 생성 다수. Claude-3.5는 비교적 안정적.
   - 국사: GPT-4/3.5는 제한된 context에서 단순 조합 문제 생성, Claude-3.5는 역사적 사실과 정답 일치 확인(을사늑약, 헤이그 특사 사건 등).
   - 클로드 3.5는 3가지의 경우 모두 만족할만한 수준으로 생성 되었음

4) 비교 분석
   - GPT-4: 문제 형식은 잘 맞추지만 본질적 다양성 부족.
   - GPT-3.5: 텍스트 반복, 잘못된 개념 포함 → 신뢰성 낮음.
   - Claude-3.5: 역사 영역에서 사실 기반 정답 제공, 상대적으로 높은 신뢰도.

5) 인사이트
   - LLM이 “문제를 이해하고 새 문제를 생성”하는 것은 아직 어려움.
   - “유사문항” 정의를 어떻게 하느냐에 따라 결과 해석이 달라짐.
   - GPT-계열보다 Claude-3.5가 교육적 활용에서 더 높은 안정성을 보임.
   - 실제 적용 시, 추가적인 검증 모듈과 문항 품질 관리 절차가 반드시 필요.
