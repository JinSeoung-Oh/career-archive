📑 프로젝트명: LLM 성능 비교 및 프롬프트 최적화 연구

기간: 2024.06 ~ 2024.07

1) 목적
   -1. GPT-3.5-turbo, GPT-4o, Haiku, HyperClova 모델을 대상으로 뉴스 분석 및 요약 Task 성능을 비교
   -2. LLM의 실제 활용 가능성을 평가하기 위해 **다양한 Task (총 6~8개)**를 설계하고 프롬프트 엔지니어링을 통해 개선 가능성을 검증

2) Task 정의
   -1. 호재/악재 분류 및 점수화 (–10 ~ 10)
       - 분류 근거를 텍스트 기반으로 상세히 서술하도록 요구
   -2. 뉴스 요약 (300자 이내)
   -3. 핵심 키워드 3개 추출 및 선정 이유 설명
   -4. 문서 분류 (광고, 시황, 공시, 뉴스) 및 근거 제시
   -5. 독자가 할 수 있는 질문 3개 생성 (각 30자 이내)
   -6. 중요 정보를 포함한 3줄 요약
   -7. (확장) 하이라이트 문장 추출 (5개)
   -8. (확장) 연관 기업 추출 (최대 10개)

3) 테스트 방법
   -1. 동일 Context를 주입한 뒤 모델별 결과를 수집
   -2. 평가 기준:
       -1) 응답의 정확성/일관성
       -2) 포맷 준수 여부 (JSON/dict 형식)
       -3) 환각 발생 여부
       -4) 토큰 처리 안정성
   -3. 한계 검증: 입력 토큰 길이 초과, 포맷 불일치, 분류 오류 등

4) 결과 요약
   (1) 모델별 성능 차이
       -1. Haiku
           - 가장 안정적으로 요구사항을 충족
           - Task 1~6 전반에서 가장 높은 객관성과 일관성 확보
           - 단점: 출력 포맷 불일치 사례(“네 질문에 답해 드리겠습니다” 등 불필요 문구 포함, JSON 미준수 사례 일부)
      -2. GPT-4o
          - GPT-3.5 대비 전반적 품질 우수
          - 답변 구조는 비교적 안정적이나, 하이라이트(Task7)에서 후처리 경향 존재
      -3. GPT-3.5-turbo
          - 전반적으로 무난하지만 Haiku와 GPT-4o 대비 세부 정확성 부족
          - Task 1에서 지나치게 부정적으로 분류하는 경향
          - Token length error 발생 사례 존재
      -4. HyperClova
          - 가장 불안정
          - 환각 발생 빈도 높음
          - Context 길이가 길어지면 에러 다수 발생
          - 요청 포맷(JSON/dict)을 준수하지 못한 사례가 전체 100건 중 97건 → 실제 사용 시 파싱 불가
   (2) Task별 주요 결과
       -1. Task 1 (호재/악재 점수화):
           -1) Haiku: 가장 합리적이고 객관적 결과
           -2) GPT-3.5: 과도하게 부정적 판단 빈번
           -3) GPT-4o: GPT-3.5보다 다소 개선
           -4) HyperClova: 환각 심각
       -2. Task 2 (요약):
           -1) Haiku: 중요 정보 대부분 포함, 가끔 과도한 디테일 포함
           -2) GPT-3.5: 일부 디테일 누락, 잘못된 요약 사례 존재
           -3) GPT-4o: GPT-3.5보다 자연스러움
       -3. Task 3 (키워드):
           -1) Haiku & GPT-3.5: 유사한 결과, 무난
           -2) HyperClova: 무난
       -4. Task 4 (분류):
           -1) GPT-3.5: 광고성 뉴스 식별 가장 잘 수행
           -2) Haiku: 광고성 뉴스 구분 불가 → 분류 기준 강화 필요
           -3) HyperClova: 무조건 뉴스로만 판단
       -5. Task 5 (질문 생성):
           -1) 모든 모델이 "Context만으로 답변 가능한 질문"만 생성 → 요구조건 상세화 필요 -- 다양한 예상 질문 생성이 목적(Context에 상관 없이)
       -6. Task 6 (3줄 요약):
           -1) Haiku: 가장 안정적
           -2) GPT-3.5: 일부 핵심 정보 누락
           -3) HyperClova: 최소 수준 충족

5) 인사이트
   -1. 프롬프트 구체화만으로도 모델 성능이 크게 개선됨을 확인
   -2. LLM 활용의 장애 요인:
       - 토큰 제한 (GPT 계열)
       - 출력 포맷 불일치 (특히 HyperClova, Haiku 일부)
       - 분류 기준 모호성 → Prompt에 정의 추가 필요
   -3. 실제 서비스 적용 시 Prompt 설계 및 출력 포맷 안정화가 성능만큼 중요함을 검증
