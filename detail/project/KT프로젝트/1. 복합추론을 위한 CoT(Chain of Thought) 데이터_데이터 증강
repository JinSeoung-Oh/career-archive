역할: 모델 검토 및 전처리 설계/구현
2024.08~2024.10

1. 프로젝트 개요
   본 프로젝트는 공공문서 및 금융문서를 기반으로 생성형 AI를 활용한 복합추론(CoT) 질의응답 데이터셋을 구축하는 것이 목적이었다.
   단순 질의응답이 아니라, 문서 내 다수의 정보를 결합하여 답을 도출하는 추론 과정까지 구조화해야 한다는 점이 핵심 요건이었다.
   또한 고객사 요구사항에 따라 QA셋을 대규모(10만 건 이상)로 생성·증강해야 했으며, 데이터 품질과 비용 최적화 모두를 고려한 설계가 필요했다.

2. 주요 기여 및 설계 요소
   -1. Context 청킹 전략 수립 및 개선
       -1) 초기 요건: 문서를 8,000~10,000음절 단위로 분할 → 지나치게 기계적이고 맥락이 단절되는 문제 발생
       -2) 개선안 제시 및 협의:
           - 문단 단위 기준으로 4,000~5,000음절 분할
           - 이전 context 하단 일부를 다음 context에 중복(800~1,200음절)하여 맥락 보존
           - 표/마크다운 처리 시 잘림 방지를 위한 정규표현식 기반 보완
       -3. 결과적으로 기계적 분할의 단점(문맥 단절)을 최소화하면서도 비용·효율성을 유지하는 타협안을 마련

2. QA 생성 설계
   -1. context당 최대 3개 QA 생성, 반드시 서로 다른 부분에 기반
   -2. 질문 조건:
       -1) 단순 fact 확인이 아닌 복합추론 요구
       -2) 문서 최소 두 곳 이상의 정보를 활용
       -3) “예/아니오/개체명”과 같은 단답형 질문은 배제
   -3. 다양성 확보: 동일 문서 내에서도 챕터별·문단별로 다른 유형의 질문 생성

3. 답변(CoT) 구조화
   -1. 고객사 정의에 맞는 CoT 단계 설계:
       -1) 행정문서: (1) 질의 재확인 → (2) 관련 문단/소제목 확인 → (3) 답변 제시 → (4) 최종 결론(옵션)
       -2) 금융문서: 정량 계산이 포함된 추론 필요
   -2. 답변은 반드시 서론–본론–결론 구조와 함께, 추론 과정이 드러나도록 작성

4. 단일/다중 문서 QA 전략
   -1. 단일 문서 QA: 전체 데이터의 80%
   -2. 다중 문서 QA: 서로 다른 문서(2~3개)에서 context 조합, 최대 15,000음절 제한, 전체의 20%
       - 다중 문서는 검색 환경을 반영해 설계

5. 데이터 증강 및 검수 체계
   -1. 원본 QA셋 대비 3배수 증강(76,722건) 수행
   -2. 증강 데이터 중 25,574건은 인력 검수 → 비용·품질 균형 고려

6. “질문-답변을 분리 생성” vs “한 번에 생성” 논의
   -1. 분리 생성: 품질 높음, 비용·시간 부담 큼
   -2. 동시 생성: 효율성 높음, 품질 저하 위험
   두 접근법 모두 테스트 후, trade-off 분석

7. 성과 및 인사이트
   -1. 대규모 한국어 QA 데이터셋 구축 프로세스 정립: 총 10만 건 이상의 복합추론 QA셋을 구조적으로 생성
   -2. 문단 오버랩 청킹 전략의 유효성 검증: 기계적 분할 대비 품질 개선 확인
   -3. CoT 데이터 다양성 확보: 단일 문서와 다중 문서를 혼합, 질문 유형과 답변 구조의 다양성 강화
   -4. 효율성 vs 품질 trade-off 분석 근거 마련: 질문-답변 동시 생성의 효율성과 분리 생성의 품질 차이를 실증

8. 한계점 및 개선 방향
   -1. 정성적 분석(시장 전망, 사회적 함의 등)에 대한 CoT 체계는 미비 → 향후 별도 설계 필요
   -2. 다중 문서 QA 활용 방안이 구체화되지 못해 실험적 수준에 머무름

API 비용 문제로 모델 선택 및 아키텍처 운영에 제약 발생

대규모 증강 데이터의 경우, 검수 범위 한정으로 인한 품질 변동성 존재
