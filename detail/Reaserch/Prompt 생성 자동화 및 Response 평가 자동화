프로젝트명: Prompt 생성 자동화 및 Response 평가 자동화
기간: 2024.04

1) 목적
  - 컨텍스트로부터 프롬프트 자동 생성 → 응답 생성 → 자동 평가까지를 하나의 파이프라인으로 구성해 사람 개입 없이 성능 점검 가능성 확인.
  - 슬로건: “머신이 만든 프롬프트에 대한 Target LLM의 response를 머신이 평가한다.”

2) 구성 요소
   -1. Prompt 생성기: GPT-3.5
   -2. Target 응답기: GPT-3.5
   -3. Judge(평가기): GPT-4.0
       -1) 채점 스킴: T/F 바이너리(정답 10점 / 오답 0점)
       -2) 근거 출력: 점수 부여 이유를 설명하도록 시스템 프롬프트에 명시
       -3) LLM-as-a-Judge 프롬프트 설계 원칙:
           - Task 설명, 스케일 정의(최소/최대/형식), 출력 포맷 고정(예: Rating: [[n]]), 답안 템플릿 시작부 제공

3) 파이프라인 흐름
   - Context 입력 → 2) GPT-3.5가 Prompt 리스트 생성
   - 각 Prompt에 대해 GPT-3.5가 응답 생성
   - GPT-4.0이 정확성 기준으로 이진 채점(10/0) + 근거 설명 출력

4) Judge 프롬프트 스니펫(요지)
   - Jailbreaking 판정 템플릿: Task 명시, 1~10 스케일, 출력 형식 고정(Rating: [[n]])
   - 일반 채점 템플릿: “Total rating: (0~10 float)” 형식, 질문·답변 페어 제공 후 피드백+총점 요구
   - MLflow 2.8 예시: ‘professionalism’ 등 기준을 정의하고 예시·스케일 제공해 자동 집계(평균/분산) 가능

5) 한계/주의 (현 단계 관찰)
   - 이진 채점(T/F)은 간단·명확하지만 부분 정답/부분 오류 구간을 구분하지 못함 → 세분 스케일이 필요할 수 있음.
   - 평가 기준 명시가 품질을 좌우: 채점 포맷 고정과 근거 요구를 통해 일관성 확보.
   - Target과 Judge의 모델/버전 분리로 상호 영향 최소화(생성=GPT-3.5, 평가=GPT-4.0).

7) 인사이트 (당신이 제공한 내용 기반에서의 정리)
   - E2E 자동 평가 가능성 확인: 컨텍스트→프롬프트→응답→채점까지 자동화가 실제로 작동함을 검증.
   - LLM-as-a-Judge 유효성: 과업 설명·스케일·포맷을 엄격히 고정하면 재현성 있는 채점이 가능.
   - 간단한 스코어링의 장단: T/F는 운영이 쉬우나, 난이도/정확도 편차를 반영하기 어렵다는 트레이드오프가 존재.
   - 프롬프트 품질→응답 품질 연동: 컨텍스트 기반 프롬프트가 명확할수록 Target 응답과 Judge 일치율이 높게 관찰.
